# Music Source Separation with U-Net

A PyTorch implementation of a U-Netâ€“based music source separator, taking stereo mixes â†’ drums, bass, other & vocals. Includes a full data pipeline (DVCâ€™ed), training, evaluation and a Gradio-powered inference server (for Hugging Face Spaces).

---
## ğŸµ Music Source Separation Demo

This Gradio Space lets you upload any mono `.wav` music mix and instantly separate it into **drums**, **bass**, **other**, and **vocals** stems using a pretrained U-Net model.

ğŸ‘‰ Try it live: https://huggingface.co/spaces/theadityamittal/music-separator-space

---

## ğŸ“‚ Repository Structure


```
.
â”œâ”€â”€ config/
â”‚   â””â”€â”€ default.yaml           # all hyperparameters & paths
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # DVC-tracked raw ZIPs & extracted WAVs
â”‚   â””â”€â”€ processed/             # DVC-tracked HDF5 spectrogram segments
â”œâ”€â”€ models/
â”‚   â””â”€â”€ checkpoints/           # saved `.pt` weights from training
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ eval.csv               # per-track SDR/SIR/SAR results
â”‚   â””â”€â”€ separated/             # WAV stems produced by evaluation
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ loader.py          # DVC + ZIP extraction & `.mp4 â†’ .wav` stems
â”‚   â”‚   â”œâ”€â”€ preprocess.py      # STFT & segmentation â†’ `.pt` or HDF5
â”‚   â”‚   â”œâ”€â”€ preprocess\_utils.py# `load_audio`, `compute_stft`
â”‚   â”‚   â”œâ”€â”€ dataset.py         # `AudioDataset` loading HDF5, lazy per-worker
â”‚   â”‚   â””â”€â”€ transforms.py      # spectrogram augmentations
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ unet.py            # U-Net architecture
â”‚   â”œâ”€â”€ train.py               # training loop (Hydra + MLflow + checkpoints)
â”‚   â””â”€â”€ serve.py               # Gradio app for inference (Hugging Face Space)
â”œâ”€â”€ dvc.yaml                   # DVC pipeline: raw ZIP â†’ processed HDF5
â”œâ”€â”€ evaluate.py                # script to compute SDR/SIR/SAR & save stems
â”œâ”€â”€ requirements.txt           # Python deps
â””â”€â”€ README.md                  # you are here

```

---

## ğŸš€ Quickstart

### 1. Install dependencies

```
python3 -m venv venv && source venv/bin/activate
pip install -r requirements.txt
```

### 2. Fetch data via DVC

```bash
dvc pull             
```

### 3. Reproduce locally

```bash
dvc repro
```

---

## ğŸ› ï¸ Training

Train the U-Net on the processed HDF5 segments, log to MLflow, and checkpoint best-validation models.

```bash
python -m src.train \
  training.epochs=50 \
  data.batch_size=16 \
  data.num_workers=4 \
  model.chans=32 \
  model.num_pool_layers=4 \
  experiment.name=full_run \
  experiment.run_name=first_training
```

For a quick smoke test on 5 batches:

```bash
python -m src.train \
  training.epochs=1 \
  training.max_steps=5 \
  data.batch_size=2 \
  data.num_workers=0 \
  experiment.name=smoke_test \
  experiment.run_name=small_batch
```

Checkpoints are saved under `models/checkpoints/` and logged as MLflow artifacts.

---

## ğŸ“Š Evaluation

Compute BSS-Eval metrics and save separated WAV stems on the **test** split:

```bash
python -m src.evaluate \
  --checkpoint models/unet_best.pt \
  --config config/default.yaml \
  --output results/eval.csv \
  --out-dir results/separated
```

* **results/eval.csv**: per-track SDR/SIR/SAR
* **results/separated/**: ground-truth stems for listening

---

## ğŸ›ï¸ Inference & Demo

Run the Gradio app locally:

```bash
python serve.py
```

Then open [http://localhost:7860](http://localhost:7860) to upload a `.wav` mix and download separated stems.

---

## â˜ Deployment on Hugging Face Spaces

1. Create a new Space, choose **Gradio** + **GPU**.
2. Push this repo (with `serve.py`, `config/`, `src/`, `requirements.txt`, `models/unet_best.pt`) to the Spaceâ€™s Git.
3. In the Space settings, set `CHECKPOINT_PATH=models/unet_best.pt`.
4. The Space will auto-build and serve a web UI.

---

## ğŸ”§ DVC Pipeline

```yaml
stages:
  preprocess:
    cmd: python -m src.data.preprocess
    deps:
      - src/data/preprocess.py
      - config/default.yaml
      - data/raw/train.zip
      - data/raw/test.zip
    outs:
      - data/processed/train.h5
      - data/processed/test.h5
```

Reproduce everything with:

```bash
dvc repro        # preprocess â†’ processed data
dvc push         # upload raw & processed artifacts
```

---

## ğŸ“ˆ Next Steps

* Improve model: complex-valued U-Net, STFT loss, phase reconstruction
* Hyperparameter sweep with Hydra + Optuna
* CI: smoke-train on PRs, linting & type checks
* Dockerize & add CI/CD for automated model serving

---

## ğŸ“œ License

[MIT License](./LICENSE)

Feel free to raise issues or contribute!
